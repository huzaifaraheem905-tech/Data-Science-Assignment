{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27bcf7d4-6859-42b0-b894-0673aace658c",
   "metadata": {},
   "source": [
    "# üßæ Titanic Survival Prediction Assignment\n",
    "## Objective:\n",
    "Predict survival of passengers on the Titanic using machine learning models (LightGBM and XGBoost). Perform preprocessing, train models, evaluate on validation set, and predict on the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de8fd65-0c3c-4816-bb01-85b53490bbed",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15ee633a-806f-4498-8442-01bba4e11cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  1. Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# ML Libraries\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "# For warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f480c456-d9c3-43a5-81db-0b12f82bece2",
   "metadata": {},
   "source": [
    "## 2. Load Training and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1f5766d-f9c2-4b8b-8425-ba690a1b8048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (891, 12)\n",
      "Test Shape: (418, 11)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  2. Load Training and Test Datasets\n",
    "file_path_1=r\"D:\\Data sciences\\Assignments\\Assignment files\\Assignment files Extracs\\XGBM & LGBM\\Titanic_train.csv\"\n",
    "\n",
    "file_path_2=r\"D:\\Data sciences\\Assignments\\Assignment files\\Assignment files Extracs\\XGBM & LGBM\\Titanic_test.csv\"\n",
    "train = pd.read_csv(file_path_1)\n",
    "test = pd.read_csv(file_path_2)\n",
    "\n",
    "print(\"Train Shape:\", train.shape)\n",
    "print(\"Test Shape:\", test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f47cfdf-9757-4563-b13f-11b667592f8a",
   "metadata": {},
   "source": [
    "### Inference:\n",
    "‚Ä¢\tTraining set has 891 rows, 12 columns.\n",
    "\n",
    "‚Ä¢\tTest set has 418 rows, 11 columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dc6a7c-1bd7-4260-8668-9dbfddace357",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing (Training Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deca9836-76b4-475e-8588-87b2d839486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  3. Data Preprocessing (Training Data)\n",
    "# Fill missing Age and Embarked values safely\n",
    "train[\"Age\"].fillna(train[\"Age\"].median(), inplace=True)\n",
    "if \"Embarked\" in train.columns:\n",
    "    train[\"Embarked\"].fillna(train[\"Embarked\"].mode()[0], inplace=True)\n",
    "    embarked_dummies = pd.get_dummies(train[\"Embarked\"], prefix=\"Embarked\", drop_first=True)\n",
    "    train = pd.concat([train, embarked_dummies], axis=1)\n",
    "    train.drop(\"Embarked\", axis=1, inplace=True)  # drop original after encoding\n",
    "\n",
    "# Encode 'Sex' column\n",
    "train[\"Sex_male\"] = train[\"Sex\"].map({\"male\": 1, \"female\": 0})\n",
    "\n",
    "# Features and target\n",
    "features = [\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Sex_male\", \"Embarked_Q\", \"Embarked_S\"]\n",
    "X = train[features]\n",
    "y = train[\"Survived\"]\n",
    "\n",
    "# Train-Validation Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcf625e-8780-452f-969d-bdfa08014183",
   "metadata": {},
   "source": [
    "### Inference:\n",
    "‚Ä¢\tMissing values handled safely.\n",
    "\n",
    "‚Ä¢\tCategorical variables encoded.\n",
    "\n",
    "‚Ä¢\tFeatures ready for model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ed92c5-689a-4a4d-914f-4efad8953607",
   "metadata": {},
   "source": [
    "## 4. Train Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3685c0c7-eb5b-4941-bd43-8e555bae1063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 273, number of negative: 439\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000719 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 206\n",
      "[LightGBM] [Info] Number of data points in the train set: 712, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383427 -> initscore=-0.475028\n",
      "[LightGBM] [Info] Start training from score -0.475028\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4. Train Models on Training Data\n",
    "# LightGBM\n",
    "lgb_model = lgb.LGBMClassifier(random_state=42)\n",
    "lgb_model.fit(X_train, y_train)\n",
    "y_lgb_val = lgb_model.predict(X_val)\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_xgb_val = xgb_model.predict(X_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667c2d73-51c4-4e6c-8a5a-3ba1693a3cf5",
   "metadata": {},
   "source": [
    "### Inference:\n",
    "‚Ä¢\tBoth LightGBM and XGBoost models are trained on the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fef18c-d9bb-47fe-9a86-3b58282be591",
   "metadata": {},
   "source": [
    "## 5. Evaluate Models on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b24552e-cf3f-49bf-b4f0-8aa76f7b2eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " LightGBM Results on Validation Set\n",
      "Accuracy: 0.7988826815642458\n",
      "Precision: 0.7538461538461538\n",
      "Recall: 0.7101449275362319\n",
      "F1 Score: 0.7313432835820896\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.85      0.84       110\n",
      "           1       0.75      0.71      0.73        69\n",
      "\n",
      "    accuracy                           0.80       179\n",
      "   macro avg       0.79      0.78      0.79       179\n",
      "weighted avg       0.80      0.80      0.80       179\n",
      "\n",
      "Confusion Matrix:\n",
      " [[94 16]\n",
      " [20 49]]\n",
      "\n",
      " XGBoost Results on Validation Set\n",
      "Accuracy: 0.8044692737430168\n",
      "Precision: 0.7575757575757576\n",
      "Recall: 0.7246376811594203\n",
      "F1 Score: 0.7407407407407407\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.85      0.84       110\n",
      "           1       0.76      0.72      0.74        69\n",
      "\n",
      "    accuracy                           0.80       179\n",
      "   macro avg       0.79      0.79      0.79       179\n",
      "weighted avg       0.80      0.80      0.80       179\n",
      "\n",
      "Confusion Matrix:\n",
      " [[94 16]\n",
      " [19 50]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  5. Evaluate Models on Validation Set\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    print(f\"\\n {model_name} Results on Validation Set\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_true, y_pred))\n",
    "    print(\"Recall:\", recall_score(y_true, y_pred))\n",
    "    print(\"F1 Score:\", f1_score(y_true, y_pred))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "\n",
    "evaluate_model(y_val, y_lgb_val, \"LightGBM\")\n",
    "evaluate_model(y_val, y_xgb_val, \"XGBoost\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d343de-0fd5-4782-89f8-ed4edb2c1451",
   "metadata": {},
   "source": [
    "### Inference:\n",
    "‚Ä¢\tBoth models perform similarly.\n",
    "\n",
    "‚Ä¢\tXGBoost has slightly higher accuracy and F1 score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3428d60f-2615-4c70-8f9e-a23ba11bccb1",
   "metadata": {},
   "source": [
    "## 6. Preprocess Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bbd3d46-fc3c-4509-bc65-ebce26b193d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  6. Preprocess Test Set safely\n",
    "test[\"Age\"].fillna(train[\"Age\"].median(), inplace=True)\n",
    "test[\"Fare\"].fillna(train[\"Fare\"].median(), inplace=True)\n",
    "test[\"Sex_male\"] = test[\"Sex\"].map({\"male\": 1, \"female\": 0})\n",
    "\n",
    "# Handle Embarked safely\n",
    "if \"Embarked\" in test.columns:\n",
    "    test[\"Embarked\"].fillna(train[[\"Embarked_Q\", \"Embarked_S\"]].mode().iloc[0], inplace=True)\n",
    "    embarked_dummies = pd.get_dummies(test[\"Embarked\"], prefix=\"Embarked\", drop_first=True)\n",
    "    test = pd.concat([test, embarked_dummies], axis=1)\n",
    "\n",
    "# Ensure all dummy columns exist\n",
    "for col in [\"Embarked_Q\", \"Embarked_S\"]:\n",
    "    if col not in test.columns:\n",
    "        test[col] = 0\n",
    "\n",
    "# Select features for test\n",
    "X_test = test[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fb72b7-6d81-453c-9b04-a8efee0b9c86",
   "metadata": {},
   "source": [
    "### Inference:\n",
    "‚Ä¢\tTest set prepared exactly like training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f534f8-103a-4c4d-b283-7ff0f41704dc",
   "metadata": {},
   "source": [
    "## 7. Predict on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df6a4e9c-8989-475c-bdb4-a4dae0ebe1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  7. Predict on Test Set\n",
    "test[\"Survived_LGB\"] = lgb_model.predict(X_test)\n",
    "test[\"Survived_XGB\"] = xgb_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f62fe58-c7db-4466-a006-8ebf20517cfa",
   "metadata": {},
   "source": [
    "## 8. Save Predictions to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f66dad8-dcb2-4fa3-8352-c30341b643ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  8. Save Predictions to CSV\n",
    "submission_lgb = test[[\"PassengerId\", \"Survived_LGB\"]].rename(columns={\"Survived_LGB\": \"Survived\"})\n",
    "submission_xgb = test[[\"PassengerId\", \"Survived_XGB\"]].rename(columns={\"Survived_XGB\": \"Survived\"})\n",
    "\n",
    "submission_lgb.to_csv(\"submission_lgb.csv\", index=False)\n",
    "submission_xgb.to_csv(\"submission_xgb.csv\", index=False)\n",
    "\n",
    "print(\"\\n Predictions saved! Files:\")\n",
    "print(\" - submission_lgb.csv\")\n",
    "print(\" - submission_xgb.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2e0e2e-27f0-4e7e-a139-8b53755481cb",
   "metadata": {},
   "source": [
    "### Inference:\n",
    "‚Ä¢\tPredictions for both models are ready.\n",
    "\n",
    "## üìù Note:\n",
    "The generated CSV files ‚Äî submission_lgb.csv and submission_xgb.csv ‚Äî are automatically saved in the same folder where this Jupyter Notebook file is running. This ensures that the output files are stored locally without needing to specify a separate path.\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
